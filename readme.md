## Веб-краулер

### Основные Критерии:
* [x] Не используется scrapy
* [x] Парсинг с помощью html.parser
* [x] Сохранение html страниц, не скачивать повторно
* [x] Фильтры для ссылок (переход в рамках указанных доменнов)
* [x] Поддержка robots.txt (Only disallow for user-agent: *)
* [x] Многопоточность
* [x] Возможность докачки (С новыми фильтрами, нужно указать точку входа)

### Некоторые фичи:
*  Кастомное количество retry
*  Кастомный таймаут
*  Поддержка/Запрет redirect (http status 301)

Usage
```shell
python main.py -r True -t 5 --url https://wikipedia.org 
```